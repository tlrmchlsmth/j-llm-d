BASE_URL := __BASE_URL__

# Use this Justfile within the cluster.

eval:
    .lm_eval/bin/lm_eval --model local-completions --tasks gsm8k \
    --model_args model={{MODEL}},base_url={{BASE_URL}}/v1/completions,num_concurrent=2000,max_retries=3,tokenized_requests=False \
    --num_fewshot 10 \

benchmark MC NUM_REQUESTS INPUT_LEN OUTPUT_LEN:
    vllm bench serve \
        --base-url {{BASE_URL}} \
        --model {{MODEL}} \
        --dataset-name random \
        --random-input-len {{INPUT_LEN}} \
        --random-output-len {{OUTPUT_LEN}}  \
        --max-concurrency {{MC}} \
        --request-rate 4096 \
        --seed $(date +%M%H%M%S) \
        --num-prompts {{NUM_REQUESTS}} \
        --ignore-eos

# Run N waves of X concurrent requests (wraps `benchmark`)
# Usage: just run-benchmark N X INPUT_LEN OUTPUT_LEN
# Optional: set OUTFILE=/path/to/log to control the log destination.
benchmark-a N X INPUT_LEN OUTPUT_LEN:
    N_X=$(( {{N}} * {{X}} ))
    outfile="${OUTFILE:-run_benchmark_{{N}}x{{X}}_$(date +%Y%m%d_%H%M%S).log}"
    just benchmark "{{X}}" "$N_X" "{{INPUT_LEN}}" "{{OUTPUT_LEN}}" |& tee "$outfile"

benchmark_g MC NUM_REQUESTS INPUT_LEN OUTPUT_LEN:
  GUIDELLM__REQUEST_TIMEOUT="900" \
  GUIDELLM__MAX_CONCURRENCY="{{MC}}" \
    guidellm benchmark \
      --target {{BASE_URL}} \
      --rate-type constant \
      --rate 2048 \
      --max-requests {{NUM_REQUESTS}} \
      --data '{"prompt_tokens": {{INPUT_LEN}}, "output_tokens": {{OUTPUT_LEN}}}'

# Simulates a pure decode workload, for measuring output token throughput and TTFT.
# Designed to approximate the decode side of a P/D deployment in a steady state.
benchmark_decode_workload NUM_REQUESTS OUTPUT_LEN:
    vllm bench serve \
        --base-url {{BASE_URL}} \
        --model {{MODEL}} \
        --dataset-name random \
        --random-input-len 1 \
        --random-output-len {{OUTPUT_LEN}}  \
        --seed $(date +%M%H%M%S) \
        --num-prompts {{NUM_REQUESTS}} \
        --ignore-eos

# For hitting an individual vLLM instance while deploying a P/D setup
benchmark_no_pd POD_IP RR NUM_REQUESTS INPUT_LEN OUTPUT_LEN:
    vllm bench serve \
        --base-url http://{{POD_IP}}:8000 \
        --model {{MODEL}} \
        --dataset-name random \
        --random-input-len {{INPUT_LEN}} \
        --random-output-len {{OUTPUT_LEN}}  \
        --request-rate {{RR}} \
        --seed $(date +%M%H%M%S) \
        --num-prompts {{NUM_REQUESTS}} \
        --ignore-eos

# For vLLM PyTorch traces.
# vLLM serve must be run with the VLLM_TORCH_PROFILER_DIR env set.
start_profile:
  curl -X POST {{BASE_URL}}/start_profile
stop_profile:
  curl -X POST {{BASE_URL}}/stop_profile
profile:
  just start_profile \
  && sleep 1 \
  && just stop_profile
