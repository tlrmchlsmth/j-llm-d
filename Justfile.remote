BASE_URL := __BASE_URL__
DECODE_POD_IPS := __DECODE_POD_IPS__

# Use this Justfile within the cluster.

default:
  just --list

eval:
    .lm_eval/bin/lm_eval --model local-completions --tasks gsm8k \
    --model_args model={{MODEL}},base_url={{BASE_URL}}/v1/completions,num_concurrent=2000,max_retries=3,tokenized_requests=False \
    --num_fewshot 10 \

benchmark MC NUM_REQUESTS INPUT_LEN OUTPUT_LEN:
    vllm bench serve \
        --base-url {{BASE_URL}} \
        --model {{MODEL}} \
        --dataset-name random \
        --random-input-len {{INPUT_LEN}} \
        --random-output-len {{OUTPUT_LEN}}  \
        --max-concurrency {{MC}} \
        --request-rate 4096 \
        --seed $(date +%M%H%M%S) \
        --num-prompts {{NUM_REQUESTS}} \
        --ignore-eos

# Run N waves of X concurrent requests (wraps `benchmark`)
# Usage: just run-benchmark N X INPUT_LEN OUTPUT_LEN
# Optional: set OUTFILE=/path/to/log to control the log destination.
benchmark-a N X INPUT_LEN OUTPUT_LEN:
    N_X=$(( {{N}} * {{X}} ))
    outfile="${OUTFILE:-run_benchmark_{{N}}x{{X}}_$(date +%Y%m%d_%H%M%S).log}"
    just benchmark "{{X}}" "$N_X" "{{INPUT_LEN}}" "{{OUTPUT_LEN}}" |& tee "$outfile"

benchmark_g MC NUM_REQUESTS INPUT_LEN OUTPUT_LEN:
  GUIDELLM__REQUEST_TIMEOUT="900" \
  GUIDELLM__MAX_CONCURRENCY="{{MC}}" \
    guidellm benchmark \
      --target {{BASE_URL}} \
      --rate-type constant \
      --rate 2048 \
      --max-requests {{NUM_REQUESTS}} \
      --data '{"prompt_tokens": {{INPUT_LEN}}, "output_tokens": {{OUTPUT_LEN}}}'

# Simulates a pure decode workload, for measuring output token throughput and TTFT.
# Designed to approximate the decode side of a P/D deployment in a steady state.
benchmark_decode_workload NUM_REQUESTS OUTPUT_LEN:
    vllm bench serve \
        --base-url {{BASE_URL}} \
        --model {{MODEL}} \
        --dataset-name random \
        --random-input-len 1 \
        --random-output-len {{OUTPUT_LEN}}  \
        --seed $(date +%M%H%M%S) \
        --num-prompts {{NUM_REQUESTS}} \
        --ignore-eos

# For hitting an individual vLLM instance while deploying a P/D setup
benchmark_no_pd POD_IP RR NUM_REQUESTS INPUT_LEN OUTPUT_LEN:
    vllm bench serve \
        --base-url http://{{POD_IP}}:8000 \
        --model {{MODEL}} \
        --dataset-name random \
        --random-input-len {{INPUT_LEN}} \
        --random-output-len {{OUTPUT_LEN}}  \
        --request-rate {{RR}} \
        --seed $(date +%M%H%M%S) \
        --num-prompts {{NUM_REQUESTS}} \
        --ignore-eos

# For vLLM PyTorch traces.
# vLLM serve must be run with the VLLM_TORCH_PROFILER_DIR env set.
start_profile URL:
  curl -X POST {{URL}}/start_profile
stop_profile URL:
  curl -X POST {{URL}}/stop_profile
profile URL:
  just start_profile {{URL}} \
  && sleep 1 \
  && just stop_profile {{URL}}

# Profile all decode pods (IPs injected during 'just poke')
profile_all_decode:
  #!/usr/bin/env bash
  set -euo pipefail

  DECODE_IPS="{{DECODE_POD_IPS}}"

  if [ -z "$DECODE_IPS" ] || [ "$DECODE_IPS" = "__DECODE_POD_IPS__" ]; then
    echo "No decode pod IPs found. Make sure you ran 'just poke' to inject the IPs."
    exit 1
  fi

  echo "Decode pod IPs: $DECODE_IPS"
  echo "Starting profile on all decode pods asynchronously..."

  for IP in $DECODE_IPS; do
    echo "  Starting profile on $IP..."
    curl -s -X POST http://$IP:8200/start_profile &
  done
  wait

  echo "All decode pods started profiling"
  echo "Sleeping for 1 second..."
  sleep 1

  echo "Stopping profile on all decode pods asynchronously..."
  for IP in $DECODE_IPS; do
    echo "  Stopping profile on $IP..."
    curl -s -X POST http://$IP:8200/stop_profile &
  done
  wait

  echo "Profiling complete on all decode pods!"
